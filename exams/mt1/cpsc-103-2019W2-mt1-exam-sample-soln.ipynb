{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 103 2019W2 Midterm 1, Sample Solution\n",
    "\n",
    "Just for fun, we'll do the sample solution to the practice exam as a Jupyter notebook. That way, you can run the code!\n",
    "\n",
    "As always, there are often many different solutions to a particular problem. Be sure you understand how and why we came to the solution we did on the exam and how to judge the quality of your own solution if it differed! (Even better: think about alternate solutions, ways to check the correctness of your solution, and alternate versions of the problem. Those are *amazingly* good study activities!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do NOT need to write imports on the exam,\n",
    "# but we do here if we want our code to run!\n",
    "\n",
    "from cs103 import *\n",
    "from typing import NamedTuple, Optional, List\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 0**\n",
    "\n",
    "Did you forget your CS alias? If so, **GO GET IT NOW** so your group doesn't get angry with you on Midterm 2! You can find it at https://www.cs.ubc.ca/getacct/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Problem 1 involves tracing Python code. So, in Jupyter it becomes \"easy\" by simply running the code, but what we really care about is how we can simulate Python's work in tracing.\n",
    "\n",
    "So, let's annotate the code blocks and then just run them for an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part a               # We'll record memory here like this: x => 5.\n",
    "                       # That would mean the variable named x has the value 5.\n",
    "    \n",
    "x = 1                  # x => 1\n",
    "y = 10                 # x => 1, y => 10\n",
    "z = 100                # x => 1, y => 10, z => 100\n",
    "result = 333           # x => 1, y => 10, z => 100, result => 333\n",
    "if x > y:              # Based on memory, this is 1 > 10, which is False.\n",
    "    result = 3*x       # so, we skip to the elif:\n",
    "elif x < y:            # This is 1 < 10, which is True. So:\n",
    "    result = 3*y       # result becomes 3*10 = 30. Memory is now:\n",
    "else:                  # x => 1, y => 10, z => 100, result => 30\n",
    "    result = 3*z       # and we skip the else branch.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part b\n",
    "\n",
    "def hundred(x):        # Remember that inside the function hundred,\n",
    "    x = 100            # there is a LOCAL x that is not the same as the one\n",
    "    return x           # outside. So, ALL this function does is ALWAYS\n",
    "                       # return 100, no matter what we pass in.\n",
    "\n",
    "x = 1                  # x => 1\n",
    "y = 10                 # x => 1, y => 10\n",
    "z = hundred(y)         # x => 1, y => 10, z => 100\n",
    "x + y + z              # so, this is 1 + 10 + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "**Problem:** Imagine you are working in Jupyter on a computer on completing the body of\n",
    "`describe_element`, a well-designed but not-yet-complete function that takes a single parameter of\n",
    "type `Element`. `Element` is an enumeration with four cases. Fill in the circles next to the two best\n",
    "choices of parts of the design that would together make it easy to complete the body.\n",
    "\n",
    "Let's look at an example. Say `Element` is from the [Avatar series](https://en.wikipedia.org/wiki/Avatar:_The_Last_Airbender) (and Legend of Korra), one of water, earth, fire, or air. `describe_element` tells us what types of things masters of these elements can control.\n",
    "\n",
    "Except... I'm **only** going to include the template function from `Element` and the examples/tests from `describe_element`:\n",
    "\n",
    "```python\n",
    "# template based on enumeration (4 cases)\n",
    "@typecheck\n",
    "def fn_for_element(e: Element) -> ...:\n",
    "    if e == Element.water:\n",
    "        return ...\n",
    "    elif e == Element.earth:\n",
    "        return ...\n",
    "    elif e == Element.fire:\n",
    "        return ...\n",
    "    elif e == Element.air:\n",
    "        return ...\n",
    "    \n",
    "start_testing()\n",
    "expect(describe_element(Element.water), \"water, ice, blood\")\n",
    "expect(describe_element(Element.earth), \"earth, lava, metal\")\n",
    "expect(describe_element(Element.fire), \"fire, lightning, explosions\")\n",
    "expect(describe_element(Element.air), \"air\")\n",
    "summary()\n",
    "```\n",
    "\n",
    "Note that our design rules insist we have exactly four tests for the four cases of `Element` here.\n",
    "\n",
    "Now, imagine you need to implement the body of this function. Can you do it? How?\n",
    "\n",
    "Sure! We copy and paste the body of `fn_for_element` and then replace each `...` with the corresponding expected value from a test:\n",
    "```python\n",
    "    if e == Element.water:\n",
    "        return \"water, ice, blood\"\n",
    "    elif e == Element.earth:\n",
    "        return \"earth, lava, metal\"\n",
    "    elif e == Element.fire:\n",
    "        return \"fire, lightning, explosions\"\n",
    "    elif e == Element.air:\n",
    "        return \"air\"\n",
    "```\n",
    "\n",
    "There are other possible answers, but none that are as good. (Indeed, just the tests alone tell me enough about the data definition to finish the whole problem if I really need, but I would have to reconstruct the template to do it. Similarly, I could have accompished the same thing with the data type definition in place of the template by reconstructing the template. I might be able to accomplish the same thing with the function's purpose in place of the tests, but I would have had to read and think much more carefully, and whether the result actually passed my tests would depend on how detailed the purpose was.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**Problem:** A user on Piazza has a display name (one or more letters and numbers) and a record of\n",
    "whether they prefer to post anonymously.\n",
    "\n",
    "Complete design of the data definition User below to represent a Piazza user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTD: You could have chosen different names for User's fields/attributes, but\n",
    "# whatever names you pick should be clear and meaningful ones.\n",
    "\n",
    "# Interp: You should say what a value is (a Piazza user) and what it's made up\n",
    "# of, connecting the fields' names to their descriptions and listing any\n",
    "# special constraints (like on the name).\n",
    "\n",
    "# Examples: Any pair of different examples would be plenty. Remember to call\n",
    "# User like a function to make them (and to follow the \"rules\" on names).\n",
    "\n",
    "User = NamedTuple('User', [('name', str),\n",
    "                           ('anon', bool)])\n",
    "# interp. a Piazza user with their display name (which must be one or more\n",
    "# letters and numbers) and whether they prefer to post anomyously (anon).\n",
    "U1 = User('wolfy314', False)\n",
    "U2 = User('mochi', True)\n",
    "\n",
    "# template based on compound (2 fields)\n",
    "@typecheck\n",
    "def fn_for_user(u: User) -> ...:\n",
    "    return ...(u.name,\n",
    "               u.anon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "**Problem:** Consider the following correct and complete data definition for a contribution’s Type (from\n",
    "the solution to the practice exam):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type = Enum('Type', ['note', 'question', 'answer'])\n",
    "# interp. a Piazza contribution's type, one of a note, question, or answer.\n",
    "# examples are redundant for enumerations\n",
    "\n",
    "@typecheck # template based on enumeration (3 cases)\n",
    "def fn_for_type(t: Type) -> ...:\n",
    "    if t == Type.note:\n",
    "        return ...\n",
    "    elif t == Type.question:\n",
    "        return ...\n",
    "    elif t == Type.answer:\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, assume the complete and correct function `is_staff` with the following signature\n",
    "and purpose is available to call.\n",
    "\n",
    "*We also \"completed\" `is_staff` enough to use in testing so we could call it in the next part! You should not have completed it, nor is it even possible to **really** complete with the information available to us!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2 of 2 tests passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def is_staff(n: str) -> bool:\n",
    "    \"\"\"\n",
    "    return True if n is the name of a course staff member and False otherwise\n",
    "    \"\"\"\n",
    "    #return True  #stub\n",
    "    #return ...(n)  #template\n",
    "    \n",
    "    # REALLY fake implementation:\n",
    "    return n == \"mochi\"\n",
    "    \n",
    "start_testing()\n",
    "expect(is_staff(\"mochi\"), True)\n",
    "expect(is_staff(\"wolfy\"), False)\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also assume that “mochi” is the name of a course staff member and “wolfy” the name of a student.\n",
    "(Do not use other names for this problem.)\n",
    "\n",
    "Now, complete the design (the stub, any remaining examples/tests, the template comment, and the\n",
    "body) of `get_rep_pts` below. Its signature and purpose are already correct and complete. For full\n",
    "credit, your design must call `is_staff` (but should not define it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m6 of 6 tests passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "@typecheck\n",
    "def get_rep_pts(t: Type, name: str) -> int:\n",
    "    \"\"\"\n",
    "    return the reputation points for a contribution of type t by a person\n",
    "    with the given name. Anyone earns 10 points for an answer and 1 for a\n",
    "    note. Staff earn 0 points for a question, while anyone else earns 10.\n",
    "    \"\"\"\n",
    "    # Now, complete the stub, add any needed examples, complete the template\n",
    "    # comment, and complete the body.\n",
    "    #return 0  #stub\n",
    "    #template from Type with additional parameter name\n",
    "    \n",
    "    if t == Type.note:\n",
    "        return 1\n",
    "    elif t == Type.question:\n",
    "        # Note that we MUST call is_staff. \n",
    "        # It is NOT correct to just compare against \"mochi\" or \"wolfy\".\n",
    "        # After all, we don't know who really is or is not staff in the end!\n",
    "        if is_staff(name):\n",
    "            return 0\n",
    "        else:\n",
    "            return 10\n",
    "    elif t == Type.answer:\n",
    "        return 10\n",
    "    \n",
    "start_testing()\n",
    "# The provided tests were ALL for staff.\n",
    "expect(get_rep_pts(Type.note,     \"mochi\"), 1)\n",
    "expect(get_rep_pts(Type.question, \"mochi\"), 0)\n",
    "expect(get_rep_pts(Type.answer,   \"mochi\"), 10)\n",
    "\n",
    "# At the very LEAST, we need one more test for non-staff on Type.question:\n",
    "expect(get_rep_pts(Type.question, \"wolfy\"), 10)\n",
    "\n",
    "# It would be fine to include these two tests as well:\n",
    "expect(get_rep_pts(Type.note,     \"wolfy\"), 1)\n",
    "expect(get_rep_pts(Type.answer,   \"wolfy\"), 10)\n",
    "\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "**Problem:** For this problem, we imagine that Piazza has three values for whether a contribution has\n",
    "been endorsed: one indicating it has been endorsed, one indicating it has not been endorsed, and one\n",
    "indicating it cannot be endorsed (because of its type). There are two good ways to represent whether\n",
    "a contribution has been endorsed: as an enumeration or an optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (a) and (b)\n",
    "\n",
    "**Problem:** Give a single brief but clear argument for why representing whether a contribution has been\n",
    "endorsed as an enumeration is a better choice than representing it as an optional. (If you give\n",
    "multiple reasons, we apply a penalty and then read only what we see as the first reason.)\n",
    "\n",
    "**Answer:** Pretty much any answer needs to recognize that an enumeration treats all three cases in some sense \"equally\" while an Optional singles out one case for special treatment&mdash;the \"cannot be endorsed\" case here&mdash;and handles the other two very naturally as a bool.\n",
    "\n",
    "Here's a plausible answer: \"An enumeration makes it natural to handle each of the three possible values using similar code.\" or even \"An enumeration lets us give clear and meaningful names to each of the three cases.\"\n",
    "\n",
    "There are many other possible correct answers! Please see the marking rubric in your returned copy for more information.\n",
    "\n",
    "Then, (b) asks us to give the DTD and interp for such a data definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endorsed = Enum('Endorsed', ['endorsed', 'not_endorsed', 'unendorsable'])\n",
    "# interp. whether a Piazza contribution has been endorsed, one of endorsed,\n",
    "# not endorsed (not_endorsed), or unendorsable if the contribution cannot\n",
    "# be endorsed because of its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts (c) and (d)\n",
    "\n",
    "**Problem:** Give a single brief but clear argument for why representing whether a contribution has been\n",
    "endorsed as an optional is a better choice than representing it as an enumeration. (If you give\n",
    "multiple reasons, we apply a penalty and then read only what we see as the first reason.)\n",
    "\n",
    "**Answer:** As above, pretty much any answer needs to recognize that an enumeration treats all three cases in some sense \"equally\" while an Optional singles out one case for special treatment&mdash;the \"cannot be endorsed\" case here&mdash;and handles the other two very naturally as a bool.\n",
    "\n",
    "Here's a plausible answer: \"An optional can handle the two normal cases (endorsed and not endorsed) using the most natural built-in type (a `bool`) by setting the third case aside as a special value.\"\n",
    "\n",
    "There are many other possible correct answers! Please see the marking rubric in your returned copy for more information.\n",
    "\n",
    "Then, (d) asks us to give the DTD and interp for such a data definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Endorsed = Optional[bool]\n",
    "# interp. whether a Piazza contribution has been endorsed, or None if the\n",
    "# contribution cannot be endorsed because of its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "**Problem:** We represent some of the information about a Piazza question using the following data\n",
    "definition (from the solution to the practice exam, modified slightly to save space on the page):\n",
    "Now, complete the design (the stub, any remaining examples/tests, template comment, and body)\n",
    "below of has_reply. Its signature and purpose are already correct and complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = NamedTuple('Question', [('number', int), # in range [1, ...]\n",
    "                                   ('has_s_answer', bool),\n",
    "                                   ('num_endorse', int), # in range [0, ...]\n",
    "                                   ('has_i_answer', bool)])\n",
    "# interp. a Piazza question with its number (no two questions in a course have\n",
    "# the same number), whether it has a student answer (has_s_answer), how many\n",
    "# endorsements the student answer has (num_endorse; 0 if there is no student\n",
    "# answer), and whether it has an instructor answer (has_i_answer).\n",
    "Q1 = Question(1, True, 4, True)\n",
    "Q2 = Question(19, False, 0, False)\n",
    "\n",
    "@typecheck # template based on compound (4 fields)\n",
    "def fn_for_question(q: Question) -> ...:\n",
    "    return ...(q.number, q.has_s_answer, q.num_endorse, q.has_i_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete the design (the stub, any remaining examples/tests, template comment, and body)\n",
    "below of `has_reply`. Its signature and purpose are already correct and complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m7 of 7 tests passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "@typecheck\n",
    "def has_reply(q: Question) -> bool:\n",
    "    \"\"\"\n",
    "    return True when q has a reply considered authoritative by the course \n",
    "    staff: an instructor answer, or a student answer with one or more\n",
    "    endorsements.\n",
    "    \"\"\"\n",
    "    #return True  #stub\n",
    "    #template from Question\n",
    "    \n",
    "    return q.has_i_answer or q.num_endorse >= 1\n",
    "\n",
    "start_testing()\n",
    "expect(has_reply(Q1), True)\n",
    "expect(has_reply(Q2), False)\n",
    "\n",
    "# We only really care about number of endorsements and whether the post has an\n",
    "# instructor answer and (sort of) a student answer. Let's include the four \n",
    "# remaining possible cases for that. Note that two more cases are impossible \n",
    "# because has_s_answer cannot be False while num_endorse > 0.\n",
    "expect(has_reply(Question(10, True, 0, True)), True)\n",
    "expect(has_reply(Question(11, True, 3, False)), True)\n",
    "expect(has_reply(Question(12, True, 0, False)), False)\n",
    "expect(has_reply(Question(13, False, 0, True)), True)\n",
    "\n",
    "# We might also want to test the \"boundary case\" where num_endorse == 1.\n",
    "# There are several such tests, we include just one of them here:\n",
    "expect(has_reply(Question(11, True, 1, False)), True)\n",
    "\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "**Part (a):** The data definition for a list of strings below contains only one mistake that causes an error in\n",
    "Python. (“One mistake” may be a set of very closely-related issues.) Running the code produces the\n",
    "following error:\n",
    "```python\n",
    "      3 LOS0 = []\n",
    "----> 4 LOS1 = [other, exam]\n",
    "      5\n",
    "    \n",
    "NameError: name 'other' is not defined\n",
    "```\n",
    "Neatly correct the mistake by crossing out if needed and writing your corrections/additions.\n",
    "(Unnecessary changes may be penalized.) [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List[str]\n",
    "# interp. a list of strings\n",
    "LOS0 = []\n",
    "LOS1 = [\"other\", \"exam\"]                # ADDED QUOTATION MARKS ON THIS LINE\n",
    "\n",
    "# template based on arbitrary-sized\n",
    "@typecheck\n",
    "def fn_for_los(los: List[str]) -> ...:\n",
    "    # description of the accumulator\n",
    "    acc = ... # type: ...\n",
    "\n",
    "    for s in los:\n",
    "        acc = ...(s, acc)\n",
    "    \n",
    "    return ...(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b):** The signature, purpose, stub, template, and single provided test for filter_modules below are\n",
    "correct and complete. Complete the design by providing any additional tests needed and neatly\n",
    "completing the body by editing the template, crossing out where needed and writing your additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m5 of 5 tests passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "@typecheck\n",
    "def filter_modules(folder_list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    returns just the module folders in folder_list. A module folder's name\n",
    "    starts with \"module\". So, given \"module1\", \"logistics\", \"module3\", and \n",
    "    \"exam\", returns just \"module1\" and \"module3\".\n",
    "    \"\"\"\n",
    "    #return [] #stub\n",
    "    #template from List[str] (with los renamed to folder_list)\n",
    "\n",
    "    # description of accumulator\n",
    "    acc = [] # type: List[str]\n",
    "\n",
    "    for f in folder_list:\n",
    "        # Designing a helper function for this would also be fine.\n",
    "        if f[:6] == \"module\":\n",
    "            #This would work instead: acc = acc + [f] \n",
    "            acc.append(f)\n",
    "\n",
    "    return acc\n",
    "\n",
    "start_testing()\n",
    "expect(filter_modules([\"module1\"]), [\"module1\"])\n",
    "\n",
    "# Ne need an empty list case:\n",
    "expect(filter_modules([]), [])\n",
    "\n",
    "# We could also use at least one case where something is NOT included,\n",
    "# at least one case with more than one element in the input list, and\n",
    "# at least one case where more than one thing starting with module\n",
    "# (probably including literally \"module\") is included/excluded. It may be\n",
    "# worthwhile to clarify what happens to, e.g., \"Module\" as well.\n",
    "\n",
    "# All this COULD be done in one test, but multiple would be better!\n",
    "expect(filter_modules([\"not_module\"]), [])\n",
    "expect(filter_modules([\"Module\"]), [])\n",
    "\n",
    "expect(filter_modules([\"module\", \"frodule\", \"module24601\", \"boo\"]), \n",
    "       [\"module\", \"module24601\"])\n",
    "\n",
    "summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
